{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "category predictor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A category predictor is used to predict the category to which a given piece of text belongs. The is frequently used in text classification to categorize text documents. Search engines frequently use this tool to order search results by relevance. For exam, let's say that we want to predict whether a given sentence belongs to sports, politics and science. To do this, we will build a corpus of data and train an algorithm. This algorithm can then be used for inference on unknown data.\n",
        "\n",
        "To buld this predictor we will use a metric called Term Frequency-Inverse Document Frequency (tf-idf). In a set of documents, we really need to understand the importance of each word. The tf-idf metric helps us to understand how important a given word is to a document in a set of documents. \n",
        "\n",
        "Now it is a time to talk about the first part of this metric. The Term Frequency (tf) is basically a measure of how frequently each word appears ina given document. Because various doments have a various number of words, the exact numbers in the histogram will variy. To have a level playing field, we need to normalize the histograms, So we divide the count of each word by the total number of words in a given document to obtain the term frequency.\n",
        "\n",
        "The second part of the metric is the Inverse Document Frequency (idf), which is a measure of how unique a word is to a document in a given set of documents, when we compute the term frequency, the assumption is that all the words are equally important. be we can not just rely on the frequency of each word because, words such as and, or , the will appear a lot. To balance the frequencies of these commonly occuring words, we will need to reduce the weights and increase the weights of the rare words. This will help us to identify words that are unique to each document as well. Which in turn helps us to formulate a distincitve feature vector. \n",
        "To compute this statistic, we need to compute the ratio of the number of documents with the given word and divide it by the total number of documents. This ratio is essentially the fraction of the documents that contain the given word. inverse document frequency is then calculated by taking the negative algorithm of this ration.\n",
        "\n",
        "We will combine term frequency and inverse document frequency to formulate a feature vector to categotrize documents. This work is the foundation for deeper analysis of the text to get deeper meaning, such as sentiment analysis, content of the text or topic analysis."
      ],
      "metadata": {
        "id": "dhLX9AxI4zX2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bno5Iah73myk"
      },
      "outputs": [],
      "source": [
        "# Import the libray\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the category map\n",
        "category_map = {'talk.politics.misc': 'Politics', 'rec.autos': 'Autos', \n",
        "        'rec.sport.hockey': 'Hockey', 'sci.electronics': 'Electronics', \n",
        "        'sci.med': 'Medicine'}\n"
      ],
      "metadata": {
        "id": "CXekyCVK8ieq"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the training dataset using fetch_20newsgroups\n",
        "training_data = fetch_20newsgroups(subset='train', \n",
        "        categories=category_map.keys(), shuffle=True, random_state=5)\n"
      ],
      "metadata": {
        "id": "Eso6xklk9AkJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We need to extract the term counts using CountVectorizer\n",
        "# We need to build a count vectorizer and extract term counts\n",
        "count_vectorizer = CountVectorizer()\n",
        "train_tc = count_vectorizer.fit_transform(training_data.data)\n",
        "print(\"\\nDimensions of training data:\", train_tc.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MQ6ZXZM9X9H",
        "outputId": "ab627217-b8d4-4430-bf6b-9ea387697e43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dimensions of training data: (2844, 40321)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the tf-idf transformer\n",
        "tfidf = TfidfTransformer()\n",
        "train_tfidf = tfidf.fit_transform(train_tc)"
      ],
      "metadata": {
        "id": "xXwX7Xn395x-"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}